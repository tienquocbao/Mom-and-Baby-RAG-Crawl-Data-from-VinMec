{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-13T13:59:15.703295Z",
     "iopub.status.busy": "2025-11-13T13:59:15.702956Z",
     "iopub.status.idle": "2025-11-13T13:59:33.390471Z",
     "shell.execute_reply": "2025-11-13T13:59:33.389596Z",
     "shell.execute_reply.started": "2025-11-13T13:59:15.703253Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.8/41.8 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.5/85.5 MB\u001b[0m \u001b[31m20.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m488.0/488.0 kB\u001b[0m \u001b[31m30.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m235.8/235.8 kB\u001b[0m \u001b[31m14.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.5/63.5 MB\u001b[0m \u001b[31m27.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m325.4/325.4 kB\u001b[0m \u001b[31m24.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m57.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m109.2/109.2 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.5/73.5 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.8/78.8 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m566.1/566.1 kB\u001b[0m \u001b[31m34.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m136.4/136.4 kB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.4/13.4 MB\u001b[0m \u001b[31m71.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m74.3/74.3 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m68.1/68.1 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m86.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25h"
     ]
    }
   ],
   "source": [
    "# CELL 1: Install packages\n",
    "!pip install -q \\\n",
    "    transformers \\\n",
    "    torch \\\n",
    "    faiss-gpu \\\n",
    "    sentence-transformers \\\n",
    "    beautifulsoup4 \\\n",
    "    tqdm \\\n",
    "    unidecode \\\n",
    "    pandas \\\n",
    "    numpy \\\n",
    "    scikit-learn \\\n",
    "    gradio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-13T13:59:33.391714Z",
     "iopub.status.busy": "2025-11-13T13:59:33.391476Z",
     "iopub.status.idle": "2025-11-13T13:59:38.849343Z",
     "shell.execute_reply": "2025-11-13T13:59:38.848272Z",
     "shell.execute_reply.started": "2025-11-13T13:59:33.391696Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.4/59.4 MB\u001b[0m \u001b[31m29.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25h"
     ]
    }
   ],
   "source": [
    "!pip -q install bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-13T13:59:38.850722Z",
     "iopub.status.busy": "2025-11-13T13:59:38.850406Z",
     "iopub.status.idle": "2025-11-13T13:59:53.598613Z",
     "shell.execute_reply": "2025-11-13T13:59:53.597868Z",
     "shell.execute_reply.started": "2025-11-13T13:59:38.850700Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n"
     ]
    }
   ],
   "source": [
    "# CELL 2: Imports & Config\n",
    "import os, re, json, random, math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from unidecode import unidecode\n",
    "from tqdm import tqdm\n",
    "from typing import List, Dict\n",
    "import faiss\n",
    "import pickle\n",
    "import torch\n",
    "\n",
    "from sentence_transformers import SentenceTransformer, CrossEncoder\n",
    "from transformers import pipeline, AutoTokenizer\n",
    "\n",
    "# === CẤU HÌNH ===\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Device: {DEVICE}\")\n",
    "\n",
    "# Model\n",
    "BI_ENCODER_MODEL = \"bkai-foundation-models/vietnamese-bi-encoder\"\n",
    "CROSS_ENCODER_MODEL = \"namdp-ptit/ViRanker\"\n",
    "LLM_MODEL = \"Qwen/Qwen2.5-1.5B-Instruct\"  # hoặc Qwen2.5-1.5B nếu GPU yếu\n",
    "\n",
    "# Crawl\n",
    "HEADERS = {\"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64)\"}\n",
    "TIMEOUT = 20\n",
    "SLEEP = 0.6\n",
    "\n",
    "# Chunking\n",
    "MAX_TOKENS = 350\n",
    "MIN_CHARS = 100\n",
    "OVERLAP_TOKENS = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-13T13:59:53.600110Z",
     "iopub.status.busy": "2025-11-13T13:59:53.599502Z",
     "iopub.status.idle": "2025-11-13T13:59:58.748934Z",
     "shell.execute_reply": "2025-11-13T13:59:58.747968Z",
     "shell.execute_reply.started": "2025-11-13T13:59:53.600085Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Đang đọc file CSV...\n",
      "Đang chunking...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4945/4945 [00:03<00:00, 1322.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Đã tạo 22,657 chunk chất lượng.\n",
      "\n",
      "Chunk đầu tiên làm ví dụ:\n",
      "{'text': 'Bệnh viêm phúc mạc ở trẻ em\\\\n\\\\nBài được viết bởi Bác sĩ Lê Văn Bình - Khoa Hồi sức tích cực - Bệnh viện Đa khoa Quốc tế Vinmec Times City. \\\\n\\\\nViêm phúc mạc là tình trạng đỏ và sưng (viêm) mô lót bụng hoặc vùng bụng của bạn. Mô này được gọi là phúc mạc. Đây có thể là một căn bệnh nguy hiểm chết người nếu không được điều trị kịp thời và đúng cách. Bệnh không chỉ xảy ra ở người trưởng thành mà còn có thể bắt gặp ở trẻ nhỏ. Tổng quan về bệnh viêm phúc mạc\\\\n\\\\nViêm phúc mạc là tình trạng viêm của lá thành - lá tạng khoang màng bụng do nguyên nhân nhiễm trùng hoặc không nhiễm trùng. \\\\n\\\\nViêm phúc mạc tiên phát. \\\\n\\\\nViêm phúc mạc thứ phát. \\\\n\\\\nViêm phúc mạc kết hợp (bệnh nhân chạy thận nhân tạo). \\\\n\\\\nNguyên nhân gây viêm phúc mạc có thể do:\\\\n\\\\nNhiễm khuẩn: Thủng ống tiêu hóa , chấn thương, tiên phát. \\\\n\\\\nKhông nhiễm khuẩn: Phẫu thuật vô trùng ổ bụng, dò dịch vô trùng vào ổ bụng, bệnh hiếm gặp,. Biểu hiện lâm sàng\\\\n\\\\nĐau bụng: Đau lan tỏa khắp bụng (giai đoạn đầu tùy nguyên nhân có thể đau khu trú). \\\\n\\\\nTriệu chứng nhiễm khuẩn: Sốt cao, mệt, nếu bệnh nhân suy kiệt có thể không sốt hoặc sốt nhẹ. \\\\n\\\\nMất nước điện giải: Môi khô, da khô, khát nước,. \\\\n\\\\nNhiễm độc: Mệt lả, da tái, xanh xao, li bì,. \\\\n\\\\nMạch, huyết áp: Mạch nhanh , nhỏ, huyết áp thường giảm, có thể có dấu hiệu sốc. \\\\n\\\\nBụng chướng ở nhiều mức độ, nhẹ đến rất căng. \\\\n\\\\nBụng kém di động theo nhịp thở, có khi không di động, cơ thành bụng có thể nổi ở trẻ lớn. \\\\n\\\\nBụng co cứng một phần hoặc toàn bộ. \\\\n\\\\nPhản ứng thành bụng (+). Càng nắn sâu càng đau. \\\\n\\\\nCảm ứng phúc mạc thường ở giai đoạn muộn. \\\\n\\\\nGõ bụng: Thường có đục ở thấp. \\\\n\\\\nThăm trực tràng: Túi cùng Douglas đầy và đau. \\\\n\\\\nChọc dò ổ bụng: Thường có dịch đục, mủ, mật, máu,.', 'title': nan, 'url': 'https://www.vinmec.com/vie/bai-viet/benh-viem-phuc-mac-o-tre-em-vi', 'chuyen_khoa': 'Nhi'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# ==========================================================\n",
    "# CELL 4 (SỬA LỖI): Semantic Chunking + Overlap\n",
    "# ==========================================================\n",
    "# --- Thêm các biến và imports còn thiếu ---\n",
    "# Giả sử các giá trị này từ các cell trước\n",
    "\n",
    "# Tên file CSV đầu vào (đã sửa đường dẫn)\n",
    "INPUT_CSV_PATH = \"/kaggle/input/seg-5k-crawl/crawled_content_from_csv.csv\"\n",
    "\n",
    "# --- Code chunking của bạn (giữ nguyên) ---\n",
    "# Tải bi_encoder (mặc dù không dùng trong logic chunking)\n",
    "# print(f\"Loading Bi-Encoder: {BI_ENCODER_MODEL}...\")\n",
    "# bi_encoder = SentenceTransformer(BI_ENCODER_MODEL, device=DEVICE)\n",
    "# print(\"Bi-Encoder loaded.\")\n",
    "\n",
    "def split_sentences(text: str) -> List[str]:\n",
    "    if not isinstance(text, str):\n",
    "        return [] # Xử lý trường hợp content_text là NaN\n",
    "    sentences = re.split(r'(?<=[.!?])\\s*\\n?\\s*|(?<=[.!?])\\s+', text)\n",
    "    return [s.strip() for s in sentences if s.strip() and len(s) > 10]\n",
    "\n",
    "def merge_small(sentences: List[str], min_chars=100) -> List[str]:\n",
    "    merged, current, chars = [], [], 0\n",
    "    for s in sentences:\n",
    "        if chars + len(s) < min_chars and current:\n",
    "            current.append(s)\n",
    "            chars += len(s) + 1\n",
    "        else:\n",
    "            if current:\n",
    "                merged.append(' '.join(current))\n",
    "            current, chars = [s], len(s)\n",
    "    if current and len(' '.join(current)) >= 50:\n",
    "        merged.append(' '.join(current))\n",
    "    return merged\n",
    "\n",
    "def semantic_chunking(text: str) -> List[Dict]:\n",
    "    sentences = merge_small(split_sentences(text), min_chars=80)\n",
    "    chunks = []\n",
    "    i = 0\n",
    "    while i < len(sentences):\n",
    "        chunk, tokens = [], 0\n",
    "        j = i\n",
    "        while j < len(sentences) and tokens + len(sentences[j].split()) <= MAX_TOKENS:\n",
    "            chunk.append(sentences[j])\n",
    "            tokens += len(sentences[j].split())\n",
    "            j += 1\n",
    "\n",
    "        chunk_text = ' '.join(chunk)\n",
    "        if len(chunk_text) >= MIN_CHARS:\n",
    "            # Overlap nếu cần\n",
    "            # (Lưu ý: logic overlap này sẽ thêm các câu *trước đó* vào *đầu* chunk hiện tại)\n",
    "            if chunks and OVERLAP_TOKENS > 0 and i >= OVERLAP_TOKENS:\n",
    "                # Tìm index câu thực tế để bắt đầu overlap\n",
    "                # Logic này hơi phức tạp, cần xem lại kỹ\n",
    "                # Giả sử 'OVERLAP_TOKENS' là số *câu* (sentences) chứ không phải *tokens*\n",
    "                overlap_sents_count = 3 # Ví dụ: overlap 3 câu\n",
    "                if i >= overlap_sents_count:\n",
    "                    overlap_sents = sentences[i - overlap_sents_count : i]\n",
    "                    chunk_text = ' '.join(overlap_sents) + ' ' + chunk_text\n",
    "\n",
    "            chunks.append({'text': chunk_text})\n",
    "        \n",
    "        # Logic bước nhảy:\n",
    "        # Nếu chunk quá nhỏ (j <= i+2), chỉ nhảy 1 câu để thử gộp\n",
    "        # Nếu chunk lớn, nhảy đến cuối chunk (j)\n",
    "        i = j if j > i + 2 else i + 1 \n",
    "    return chunks\n",
    "\n",
    "# --- Áp dụng chunking (ĐÃ SỬA LỖI) ---\n",
    "print(\"Đang đọc file CSV...\")\n",
    "try:\n",
    "    df = pd.read_csv(INPUT_CSV_PATH)\n",
    "    \n",
    "    print(\"Đang chunking...\")\n",
    "    all_chunks = []\n",
    "    \n",
    "    # Sử dụng tqdm(df.iterrows(), total=df.shape[0])\n",
    "    for _, row in tqdm(df.iterrows(), total=df.shape[0]):\n",
    "        chunks = semantic_chunking(row['content_text']) # Đọc từ 'content_text'\n",
    "        for c in chunks:\n",
    "            all_chunks.append({\n",
    "                'text': c['text'],\n",
    "                # === SỬA LỖI Ở ĐÂY ===\n",
    "                # Sử dụng tên cột viết thường từ file CSV\n",
    "                'title': row['title'],\n",
    "                'url': row['url'],\n",
    "                'chuyen_khoa': row['chuyen_khoa']\n",
    "                # =====================\n",
    "            })\n",
    "\n",
    "    print(f\"Đã tạo {len(all_chunks):,} chunk chất lượng.\")\n",
    "    \n",
    "    # In thử 1 chunk để xem kết quả\n",
    "    if all_chunks:\n",
    "        print(\"\\nChunk đầu tiên làm ví dụ:\")\n",
    "        print(all_chunks[0])\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"LỖI: Không tìm thấy file {INPUT_CSV_PATH}\")\n",
    "except Exception as e:\n",
    "    print(f\"Đã xảy ra lỗi: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-13T14:03:03.618770Z",
     "iopub.status.busy": "2025-11-13T14:03:03.618460Z",
     "iopub.status.idle": "2025-11-13T14:03:32.645385Z",
     "shell.execute_reply": "2025-11-13T14:03:32.644688Z",
     "shell.execute_reply.started": "2025-11-13T14:03:03.618746Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Đang tải model trên cuda...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "249a41849c03418fa652ebb9628e5b9a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/229 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c3b29e0d4474ea5a53b746946481401",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/123 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1bca409b0d6b442a8ce8ebb90e2df363",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3249db855d634033a97be53f645b1d6f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e3dee9b2631141b98a2fe9481ac786a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/777 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "32fb557c2323403f9eecedf3f8fecea5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/540M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "61c0d2169230493e8fb56de7dcc88147",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0495a89c2d2d4426b0fdc1c46fdc9cb4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "456934374f0d4cab8dc4ccab43ff6bd4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "bpe.codes: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6eba9ecd7a9448e6946ed4aa9fe4412c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "added_tokens.json:   0%|          | 0.00/22.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e1045da87e246d284b50da968d39074",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/167 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7729f0cdd5ba409fbccaabf9bd37cd3e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/270 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "54f20a027a9a4c679d3bb16f1b3d2800",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/796 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7464e547bbbe4d4d9701e38e1655a178",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/2.27G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "77853f0f663e43858eca485accd1f0aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "749dcbf2dd4c4a019b5e325da3654a85",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentencepiece.bpe.model:   0%|          | 0.00/5.07M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd39c82fc52b45cead4494ada0bd864c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/17.1M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "78362893fb6944f789dfce1f9372d12a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/964 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd4c870136494e19b300e31a13d205e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a618cdbef3fb4facb1e43167e7721371",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "795a6c8bb9244cd5a253c606caf951cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "60968f67f71c4e368400b8f6c43ba4d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1eebc5874d2f4452b869fe3d21568eea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4025a4145b28469b80b96926f2588fcc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/660 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "686a84ca5d8848139251ff623452d337",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/3.09G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0eb80053a33d4d0791883f7090f69644",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/242 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ==========================================================\n",
    "# Bước 3: Tải Model và Dữ liệu (chỉ tải 1 lần)\n",
    "# ==========================================================\n",
    "print(f\"Đang tải model trên {DEVICE}...\")\n",
    "bi_encoder = SentenceTransformer(BI_ENCODER_MODEL, device=DEVICE)\n",
    "cross_encoder = CrossEncoder(CROSS_ENCODER_MODEL, device=DEVICE)\n",
    "llm_tokenizer = AutoTokenizer.from_pretrained(LLM_MODEL)\n",
    "generator = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=LLM_MODEL,\n",
    "    tokenizer=llm_tokenizer,\n",
    "    model_kwargs={\"torch_dtype\": torch.bfloat16},\n",
    "    device_map=DEVICE,\n",
    "    pad_token_id=llm_tokenizer.eos_token_id,\n",
    "    eos_token_id=llm_tokenizer.eos_token_id,\n",
    "    max_new_tokens=512\n",
    "    #\"load_in_8bit\": True\n",
    "     )\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-13T14:03:57.775583Z",
     "iopub.status.busy": "2025-11-13T14:03:57.775258Z",
     "iopub.status.idle": "2025-11-13T14:07:19.023889Z",
     "shell.execute_reply": "2025-11-13T14:07:19.023025Z",
     "shell.execute_reply.started": "2025-11-13T14:03:57.775555Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tạo embedding...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4202d2ffd8784f19a156e191c1830648",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/355 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FAISS + 22657 chunks đã lưu!\n"
     ]
    }
   ],
   "source": [
    "# CELL 5: Build FAISS\n",
    "print(\"Tạo embedding...\")\n",
    "embeddings = bi_encoder.encode(\n",
    "    [c['text'] for c in all_chunks],\n",
    "    batch_size=64,\n",
    "    show_progress_bar=True,\n",
    "    convert_to_numpy=True,\n",
    "    normalize_embeddings=True\n",
    ").astype('float32')\n",
    "\n",
    "dimension = embeddings.shape[1]\n",
    "index = faiss.IndexFlatIP(dimension)\n",
    "index.add(embeddings)\n",
    "\n",
    "# Lưu\n",
    "faiss.write_index(index, \"vinmec_faiss_token_chunk.index\")\n",
    "with open(\"vinmec_chunks.pkl\", \"wb\") as f:\n",
    "    pickle.dump(all_chunks, f)\n",
    "\n",
    "print(f\"FAISS + {len(all_chunks)} chunks đã lưu!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-13T14:08:41.787847Z",
     "iopub.status.busy": "2025-11-13T14:08:41.787520Z",
     "iopub.status.idle": "2025-11-13T14:08:48.629412Z",
     "shell.execute_reply": "2025-11-13T14:08:48.628698Z",
     "shell.execute_reply.started": "2025-11-13T14:08:41.787819Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# CELL 6: RAG System\n",
    "cross_encoder = CrossEncoder(CROSS_ENCODER_MODEL, device=DEVICE)\n",
    "\n",
    "# Load FAISS + chunks\n",
    "index = faiss.read_index(\"/kaggle/input/seg-5k-crawl/vinmec_faiss_token_chunk.index\")\n",
    "with open(\"/kaggle/input/seg-5k-crawl/vinmec_chunks.pkl\", \"rb\") as f:\n",
    "    chunks = pickle.load(f)\n",
    "\n",
    "# LLM\n",
    "generator = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=LLM_MODEL,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    "    max_new_tokens=300,\n",
    "    temperature=0.3,\n",
    "    do_sample=True\n",
    ")\n",
    "\n",
    "def retrieve(query: str, k=30) -> List[Dict]:\n",
    "    q_emb = bi_encoder.encode([query], normalize_embeddings=True)\n",
    "    D, I = index.search(q_emb, k)\n",
    "    return [chunks[i] for i in I[0]]\n",
    "\n",
    "def rerank(query: str, docs: List[Dict], topk=5) -> List[Dict]:\n",
    "    pairs = [[query, d['text']] for d in docs]\n",
    "    scores = cross_encoder.predict(pairs)\n",
    "    ranked = sorted(zip(docs, scores), key=lambda x: x[1], reverse=True)\n",
    "    return [r[0] for r in ranked[:topk]]\n",
    "\n",
    "def rag_answer(query: str) -> str:\n",
    "    docs = retrieve(query, k=30)\n",
    "    top_docs = rerank(query, docs, topk=5)\n",
    "\n",
    "    context = \"\\n\\n\".join([\n",
    "        f\"[{d['title']}]: {d['text'][:1200]}\"\n",
    "        for d in top_docs\n",
    "    ])\n",
    "\n",
    "    prompt = f\"\"\"Bạn là bác sĩ chuyên khoa Nhi và Phụ nữ.\n",
    "Dựa vào thông tin sau, trả lời ngắn gọn, dễ hiểu:\n",
    "\n",
    "{context}\n",
    "\n",
    "Câu hỏi: {query}\n",
    "Trả lời:\"\"\"\n",
    "\n",
    "    output = generator(prompt)[0]['generated_text']\n",
    "    answer = output.split(\"Trả lời:\")[-1].strip()\n",
    "    return answer, top_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-13T14:08:58.127529Z",
     "iopub.status.busy": "2025-11-13T14:08:58.127243Z",
     "iopub.status.idle": "2025-11-13T14:08:59.621098Z",
     "shell.execute_reply": "2025-11-13T14:08:59.620113Z",
     "shell.execute_reply.started": "2025-11-13T14:08:58.127507Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "bi_encoder = SentenceTransformer(BI_ENCODER_MODEL, device=DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-13T14:09:01.612907Z",
     "iopub.status.busy": "2025-11-13T14:09:01.612611Z",
     "iopub.status.idle": "2025-11-13T14:09:15.128036Z",
     "shell.execute_reply": "2025-11-13T14:09:15.127220Z",
     "shell.execute_reply.started": "2025-11-13T14:09:01.612884Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "332d21014dc84551946411d5f38fb63b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "97c20174a7d64b75b3d17ac945cb81a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CÂU HỎI: Ung thư tử cung\n",
      "\n",
      "TRẢ LỜI:\n",
      "Ung thư cổ tử cung là một loại ung thư xảy ra trong các tế bào của cổ tử cung. Nguyên nhân chính gây ra bệnh này là virus papilloma ở người (HPV), đây là bệnh lây truyền qua đường tình dục. Người bệnh có thể giảm nguy cơ phát triển ung thư cổ tử cung bằng cách khám sàng lọc và tiêm vắc-xin chống nhiễm trùng HPV. Ung thư cổ tử cung thường phát triển chậm, vì vậy, người bệnh có thời gian để phát hiện và điều trị trước khi nó gây ra các vấn đề nghiêm trọng hơn. \n",
      "\n",
      "Phương pháp điều trị thông thường có thể kể đến là hóa trị và xạ trị. Nếu bệnh nhân kiểm tra dương tính với đột biến gen BRCA, phẫu thuật cắt tử cung có thể không cần thiết. Thay vào đó, các bác sĩ có thể đề xuất loại bỏ buồng trứng và ống dẫn trứng vì bệnh nhân đột biến gen này có nguy cơ cao mắc phải ung thư buồng trứng, ung thư vú. \n",
      "\n",
      "Bệnh lạc nội mạc tử cung là một dạng rối loạn, là tình trạng mô lót bên trong niêm mạc tử cung phát triển không bình thường, lan rộng ra các khu vực bên ngoài. Tình trạng này thường gây đau dữ dội và kinh nguyệt không đều, có thể dẫn đến vô sinh. Thông thường, các bác sĩ khuyên người bệnh nên\n",
      "\n",
      "NGUỒN:\n",
      "- nan (https://www.vinmec.com/vie/bai-viet/nguyen-nhan-gay-ra-benh-ung-thu-co-tu-cung-la-gi-vi)\n",
      "- nan (https://www.vinmec.com/vie/bai-viet/ung-thu-co-tu-cung-co-may-giai-doan-va-cach-dieu-tri-vi)\n",
      "- nan (https://www.vinmec.com/vie/bai-viet/pap-smear-sinh-thiet-ct-mri-trong-chan-doan-ung-thu-co-tu-cung-vi)\n",
      "- nan (https://www.vinmec.com/vie/bai-viet/ung-thu-co-tu-cung-co-may-giai-doan-va-cach-dieu-tri-vi)\n",
      "- nan (https://www.vinmec.com/vie/bai-viet/cat-tu-cung-11-benh-ly-lien-quan-va-nhung-dieu-can-biet-ve-phuong-phap-dieu-tri)\n"
     ]
    }
   ],
   "source": [
    "# CELL 7: Test\n",
    "query = \"Ung thư tử cung\"\n",
    "answer, sources = rag_answer(query)\n",
    "\n",
    "print(\"CÂU HỎI:\", query)\n",
    "print(\"\\nTRẢ LỜI:\")\n",
    "print(answer)\n",
    "print(\"\\nNGUỒN:\")\n",
    "for s in sources:\n",
    "    print(f\"- {s['title']} ({s['url']})\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 8682557,
     "sourceId": 13657226,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30823,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
